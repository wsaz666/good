import os
import torch
import numpy as np
import sacrebleu
from datasets import load_dataset
from transformers import (
    GPT2LMHeadModel,
    GPT2TokenizerFast,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)

# ================= 1. å…¨å±€é…ç½®ä¸èµ„æºåŠ è½½ =================
# ä¸ºäº†é˜²æ­¢ Windows å¤šè¿›ç¨‹æŠ¥é”™ï¼Œæˆ‘ä»¬å°† tokenizer åœ¨å…¨å±€åŠ è½½
# è¿™æ ·å­è¿›ç¨‹ import è„šæœ¬æ—¶ä¹Ÿèƒ½ç›´æ¥è·å–åˆ° tokenizer å¯¹è±¡
tokenizer_path = "./verilog_tokenizer_32k"
model_stage1_path = "./verilog_gpt2_stage1_pretrain/checkpoint-8595"
dataset_path = "D:/code/code generation/data/train_platinum_distilled.csv"
output_dir = "./verilog_gpt2_stage2_sft"

print(f"æ­£åœ¨åŠ è½½åˆ†è¯å™¨: {tokenizer_path} ...")
try:
    tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_path)
    tokenizer.pad_token = "<|padding|>"
    tokenizer.eos_token = "<|endoftext|>"
except Exception as e:
    print(f"âš ï¸ å…¨å±€åˆ†è¯å™¨åŠ è½½è­¦å‘Š: {e}")
    tokenizer = None

# ================= 2. æ ¸å¿ƒå‡½æ•° (å®šä¹‰åœ¨å…¨å±€) =================

def preprocess_function(examples, tokenizer):
    # æ³¨æ„ï¼šè¿™é‡Œæ¥æ”¶é€šè¿‡ fn_kwargs ä¼ å…¥çš„ tokenizer
    inputs = examples["instruction"]
    outputs = examples["output"]

    new_texts = []
    for inst, out in zip(inputs, outputs):
        # SFT æ ¼å¼: <instruction>: ... \n <output>: ... <|endoftext|>
        text = f"<instruction>: {inst}\n<output>: {out}{tokenizer.eos_token}"
        new_texts.append(text)

    return tokenizer(
        new_texts,
        truncation=True,
        max_length=1024,
    )


def preprocess_logits_for_metrics(logits, labels):
    """
    æ˜¾å­˜ä¼˜åŒ–ï¼šåœ¨ GPU ä¸Šå³æ—¶å°† Logits é™ç»´ä¸º Token IDã€‚
    """
    if isinstance(logits, tuple):
        logits = logits[0]
    return logits.argmax(dim=-1)


def compute_metrics(eval_preds):
    preds, labels = eval_preds

    # ================= ä¿®å¤å¼€å§‹ =================
    # 1. å¦‚æœ preds æ˜¯å…ƒç»„ï¼ˆæœ‰äº›æ¨¡å‹ä¼šè¾“å‡º tupleï¼‰ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
    if isinstance(preds, tuple):
        preds = preds[0]

    # 2. ã€å…³é”®ä¿®å¤ã€‘å°†é¢„æµ‹ç»“æœä¸­çš„ -100 æ›¿æ¢ä¸º pad_token_id
    # Trainer ä¼šåœ¨ batch å¯¹é½æ—¶è‡ªåŠ¨å¡«å…¥ -100ï¼Œè¿™ä¼šå¯¼è‡´ tokenizer æŠ¥é”™ï¼
    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)

    # 3. å°†æ ‡ç­¾ä¸­çš„ -100 ä¹Ÿæ›¿æ¢ä¸º pad_token_id (ä¸ºäº†è§£ç ä¸æŠ¥é”™)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # ================= ä¿®å¤ç»“æŸ =================

    # è§£ç 
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # å»é™¤ç©ºæ ¼
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = {}

    # === BLEU è®¡ç®— (ä½¿ç”¨åŸç”Ÿ sacrebleu) ===
    import sacrebleu
    try:
        # ç®€å•æ‰“å°ä¸€æ¡çœ‹çœ‹ï¼Œç¡®ä¿è§£ç æ­£å¸¸
        # print(f"[Debug] Pred: {decoded_preds[0][:50]}...")

        # sacrebleu.corpus_bleu éœ€è¦ references æ˜¯ [[ref1_a, ref2_a], [ref1_b, ref2_b]]
        # æˆ‘ä»¬åªæœ‰å•å‚è€ƒï¼Œæ‰€ä»¥éœ€è¦è½¬ç½®ä¸€ä¸‹ï¼š [decoded_labels_clean] -> [[label1, label2, ...]]
        # æ³¨æ„ï¼šdecoded_labels ç›®å‰æ˜¯ [['label1'], ['label2']]
        # æˆ‘ä»¬éœ€è¦æŠŠå®ƒå˜æˆ [['label1', 'label2', ...]] çš„å½¢å¼ç»™ corpus_bleu

        # ä¿®æ­£ references çš„æ ¼å¼
        refs = [[l[0] for l in decoded_labels]]

        bleu = sacrebleu.corpus_bleu(decoded_preds, refs)
        result["bleu"] = bleu.score
    except Exception as e:
        print(f"âš ï¸ BLEU è®¡ç®—æŠ¥é”™: {e}")
        result["bleu"] = 0.0
    # ======================================

    # è®¡ç®— Token Accuracy
    mask = labels != tokenizer.pad_token_id
    correct = (preds == labels) & mask
    accuracy = correct.sum() / mask.sum() if mask.sum() > 0 else 0
    result["token_accuracy"] = accuracy

    return result


# ================= 3. ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    # æ˜¾å¡æ£€æŸ¥
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # äºŒæ¬¡æ£€æŸ¥ Tokenizer
    if tokenizer is None:
        print("âŒ åˆ†è¯å™¨æœªæ­£ç¡®åŠ è½½ï¼Œç¨‹åºé€€å‡ºã€‚")
        exit()

    # --- åŠ è½½æ¨¡å‹ ---
    print(f"æ­£åœ¨åŠ è½½é˜¶æ®µä¸€æ¨¡å‹: {model_stage1_path} ...")
    try:
        model = GPT2LMHeadModel.from_pretrained(model_stage1_path)
        model.to(device)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        exit()

    # --- æ•°æ®å¤„ç† ---
    print(f"æ­£åœ¨åŠ è½½æ•°æ®: {dataset_path} ...")
    dataset = load_dataset("csv", data_files=dataset_path)["train"]

    print("æ­£åœ¨æ„å»ºå¯¹è¯æ•°æ®...")
    # Windows ä¸‹ä½¿ç”¨ map ä¸” num_proc > 1 æ—¶ï¼Œ
    # è°ƒç”¨çš„å‡½æ•° preprocess_function å¿…é¡»æ˜¯å…¨å±€å®šä¹‰çš„
    tokenized_dataset = dataset.map(
        preprocess_function,
        batched=True,
        num_proc=4,
        remove_columns=dataset.column_names,
        fn_kwargs={"tokenizer": tokenizer}  # ã€å…³é”®ã€‘æ˜¾å¼ä¼ é€’ tokenizer
    )

    split = tokenized_dataset.train_test_split(test_size=2000, seed=42)
    train_dataset = split["train"]
    eval_dataset = split["test"]

    # --- è®­ç»ƒé…ç½® ---
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,
        learning_rate=5e-5,
        lr_scheduler_type="cosine",
        warmup_ratio=0.03,
        fp16=True,
        logging_steps=50,
        save_strategy="epoch",
        eval_strategy="epoch",
        save_total_limit=2,
        report_to="none"
    )

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    # --- åˆå§‹åŒ– Trainer ---
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics,
        compute_metrics=compute_metrics
    )

    # --- å¼€å§‹è®­ç»ƒ ---
    print("ğŸš€ å¼€å§‹é˜¶æ®µäºŒæŒ‡ä»¤å¾®è°ƒ (SFT)...")
    trainer.train()
    print("ğŸ‰ å®Œæˆï¼")