import os
import torch
from datasets import load_dataset
from transformers import (
    GPT2Config,
    GPT2LMHeadModel,
    GPT2TokenizerFast,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)

# ================= 1. é…ç½®è·¯å¾„ =================
tokenizer_path = "./verilog_tokenizer_32k"
dataset_path = "D:/code/code generation/data/train_final_eos.csv"
output_dir = "./verilog_gpt2_stage1_pretrain"


# ================= é¢„å¤„ç†å‡½æ•°å®šä¹‰ =================
# æ³¨æ„ï¼šå‡½æ•°å®šä¹‰å¿…é¡»æ”¾åœ¨ä¸»ç¨‹åºå¤–éƒ¨ï¼Œä»¥ä¾¿å¤šè¿›ç¨‹è°ƒç”¨
def get_preprocess_function(tokenizer):
    def preprocess_function(examples):
        return tokenizer(
            examples["content"],
            truncation=True,
            max_length=1024,
        )

    return preprocess_function


# ================= ä¸»ç¨‹åºå…¥å£ =================
def main():
    # æ˜¾å¡é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # ================= 2. åŠ è½½åˆ†è¯å™¨ =================
    print(f"æ­£åœ¨åŠ è½½ä¸“ç”¨åˆ†è¯å™¨: {tokenizer_path} ...")
    try:
        tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_path)
        tokenizer.pad_token = "<|padding|>"
        tokenizer.eos_token = "<|endoftext|>"
        tokenizer.bos_token = tokenizer.eos_token
    except Exception as e:
        print(f"åŠ è½½åˆ†è¯å™¨å¤±è´¥ï¼è¯·å…ˆè¿è¡Œ train_new_setup.py ç”Ÿæˆåˆ†è¯å™¨ã€‚\né”™è¯¯: {e}")
        return

    # ================= 3. åˆå§‹åŒ–æ¨¡å‹ =================
    print("æ­£åœ¨åˆå§‹åŒ– GPT-2 (12å±‚/768ç»´) ...")
    config = GPT2Config(
        vocab_size=len(tokenizer),
        n_positions=1024,
        n_ctx=1024,
        n_embd=768,
        n_layer=12,
        n_head=12,
        bos_token_id=tokenizer.bos_token_id,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id,
        use_cache=False,
        gradient_checkpointing=True
    )

    model = GPT2LMHeadModel(config)
    model.to(device)
    print(f"æ¨¡å‹å‚æ•°é‡: {model.num_parameters() / 1e6:.2f} M")

    # ================= 4. æ•°æ®åŠ è½½ä¸å¤„ç† =================
    print(f"æ­£åœ¨åŠ è½½æ•°æ®: {dataset_path} ...")
    dataset = load_dataset("csv", data_files=dataset_path)["train"]

    print("æ­£åœ¨å¯¹æ•°æ®è¿›è¡Œ Tokenize (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")

    # è·å–ç»‘å®šäº† tokenizer çš„å¤„ç†å‡½æ•°
    process_func = get_preprocess_function(tokenizer)

    # ã€å…³é”®ä¿®æ”¹ã€‘å¤šè¿›ç¨‹å¤„ç†å¿…é¡»åœ¨ main å—ä¿æŠ¤ä¸‹è¿è¡Œ
    tokenized_dataset = dataset.map(
        process_func,
        batched=True,
        num_proc=4,  # Windows ä¸‹è¿™é‡Œä¼šè§¦å‘ spawn
        remove_columns=["content"]
    )

    # åˆ’åˆ†éªŒè¯é›†
    split_dataset = tokenized_dataset.train_test_split(test_size=5000, seed=42)
    train_dataset = split_dataset["train"]
    eval_dataset = split_dataset["test"]

    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(eval_dataset)}")

    # ================= 5. è®­ç»ƒå‚æ•°é…ç½® =================
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=5,             #æŠŠæ•°æ®å®Œæ•´çœ‹5é
        per_device_train_batch_size=4,  #æ˜¾å¡ä¸€æ¬¡è¯»4æ¡æ•°æ®
        gradient_accumulation_steps=8,  #ç´¯è®¡8æ¬¡æ›´æ–°å‚æ•°
        learning_rate=2e-4,
        lr_scheduler_type="cosine",
        warmup_ratio=0.03,
        fp16=True,
        logging_steps=50,
        save_strategy="epoch",
        eval_strategy="epoch",
        save_total_limit=2,
        report_to="none"
    )

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )

    # ================= 6. å¼€å§‹è®­ç»ƒ =================
    print("ğŸš€ å¼€å§‹é˜¶æ®µä¸€é¢„è®­ç»ƒ (Pre-training)...")
    trainer.train()
    print("ğŸ‰ é˜¶æ®µä¸€è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³:", output_dir)


if __name__ == "__main__":
    # Windows å¿…é¡»åŠ è¿™ä¸€è¡Œæ¥é˜²æ­¢å¤šè¿›ç¨‹é€’å½’æŠ¥é”™
    main()